% This is a sample document using the University of Minnesota, Morris, Computer Science
% Senior Seminar modification of the ACM sig-alternate style. Much of this content is taken
% directly from the ACM sample document illustrating the use of the sig-alternate class. Certain
% parts that we never use have been removed to simplify the example, and a few additional
% components have been added.

% See https://github.com/UMM-CSci/Senior_seminar_templates for more info and to make
% suggestions and corrections.

\documentclass{sig-alternate}
\usepackage{color}
%%%\usepackage[colorinlistoftodos]{todonotes}

%%%%% Uncomment the following line and comment out the previous one
%%%%% to remove all comments
%%%%% NOTE: comments still occupy a line even if invisible;
%%%%% Don't write them as a separate paragraph
%\newcommand{\mycomment}[1]{}

\begin{document}

% --- Author Metadata here ---
%%% REMEMBER TO CHANGE THE SEMESTER AND YEAR AS NEEDED
\conferenceinfo{UMM CSci Senior Seminar Conference, December 2015}{Morris, MN}

\title{Document Labeling and Topic Evolution Using Social Media}

\numberofauthors{1}

\author{
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
\alignauthor
Zachary D. Vink\\
	\affaddr{Division of Science and Mathematics}\\
	\affaddr{University of Minnesota, Morris}\\
	\affaddr{Morris, Minnesota, USA 56267}\\
	\email{vinkx009@morris.umn.edu}
}

\maketitle
%\begin{abstract}
%\end{abstract}

\keywords{Will be decided later}

%\section{Introduction}
%\label{sec:introduction}

%\section{Background}
%\label{sec:body}

%\section{Latent Dirichlet Allocation}
%\label{sec:latentDirichletAllocation}

%\section{The Badge Model}
%\label{theBadgeModel}

\section{Learning Topic Evolution}
\label{sec:topicEvolution}

The amount of documents shared through social media increase the 
potential for studying topic evolution. 
Topic evolution expands on document labeling. 
The primary method this paper covers is called Learning Topic Evolution from Content and Social media activity (LTECS). 
LTECS uses Non-negative Matrix Factorization for labeling documents. 
However, compared to LDA and The Badge Model (covered in sections 3 and 4) LTECS implements collective factorization in order to make predictions off of two matrices at the same time. 
LTECS implements collective factorization in order to allow the option of describing documents through either a trained corpus of labels or a community of users. 
In conjunction with data over time, LTECS uses this information to justify if a document is community-stable or content-stable. 
Community-stable and content stable are covered further in section 6.

To begin understanding LTECS, we look at the necessary matrices. 
The matrices \textbf{X}\textsuperscript{t} and \textbf{U}\textsuperscript{t} denote two matrices defined at time \textit{t}.
The matrix \textbf{X}\textsuperscript{t} is a \textit{N}$_d^t$ x \textit{N}$_f$ matrix at time \textit{t} composed of \textit{N}$_d^t$ documents and \textit{N}$_f$ textual features.
Each row in \textbf{X}\textsuperscript{t} represents a single document that was shared (through an arbitrary social media site) at time \textit{t}. 
The amount of time \textit{t} represents ranges from a month to a year based on the preference when organizing the data.
The textual features that compose the columns in \textbf{X}\textsuperscript{t} represent a variety attributes semi-unique to the given document. 
Here, semi-unique is used to describe an attribute that is rare enough among all the documents that it has a possible impact on the actual identity of the article.
The matrix, when created, is a variety of weights that show which labels are associated to a given document. 
\textbf{U}\textsuperscript{t} is similar to \textbf{X}\textsuperscript{t} except the textual features are replaced by users. 
The result is a \textit{N}$_d^t$ x \textit{N}$_u$ matrix. 
The documents used to describe \textbf{U}\textsuperscript{t} and \textbf{X}\textsuperscript{t} are the same documents in the same arbitrary order. 
However, the amount of textual features for \textbf{X}\textsuperscript{t} does not need to match the number of users for  \textbf{U}\textsuperscript{t}.

As mentioned previously, \textbf{U}\textsuperscript{t} and \textbf{X}\textsuperscript{t} are both used by LTECS to label topics. 
To use these matrices for topic discovery LTECS uses the standard Non-negative Matrix Factorization (NMF) technique. 
NMF is used to decompose documents into a variety of latent topics. 
A latent topic is a label for a document that is inferred by other pieces of data associated with the document. 
For example, if an article is shared on Facebook and all of the comments below the article mention Will Smith, then we could justify that the article is about Will Smith. 
Making Will Smith the latent topic. 
NMF identifies latent topics by having a trained matrix that associates features with a given topic. 
LTECS uses a trained matrix \textbf{H}\textsuperscript{t} to define \textbf{X}\textsuperscript{t}.

\begin{equation}
	\textrm{\textbf{X}}^t\approx \textrm{\textbf{W}}^t\textrm{\textbf{H}}^t 
\label{eq:featuresMatix}
\end{equation}

In equation 1, \textbf{H}\textsuperscript{t} is a \textit{k} x \textit{N}$_f$ trained matrix used to create \textbf{W}\textsuperscript{t}. 
To allow for more usability, \textit{k} can be set to any number upon training to be equal to however many topics we wish to choose from when defining documents. 
\textbf{W}\textsuperscript{t} is a \textit{N}$_d^t$ x \textit{k} matrix. 
This matrix associates a document with a variety of topics based on the highest weights in the matrix.
The matrix representations below show how these matrices are filled.
\[
  \stackrel{\mbox{\textbf{W}$^t$}}{%
    \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1k} \\
    a_{21} & a_{22} & \cdots & a_{2k} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{N_d1} & a_{N_d2} & \cdots & a_{N_dk}
    \end{bmatrix}%
  }
  \stackrel{\mbox{\textbf{H}$^t$}}{%
    \begin{bmatrix}
    b_{11} & b_{12} & \cdots & b_{1N_f} \\
    b_{21} & b_{22} & \cdots & b_{2N_f} \\
    \vdots & \vdots & \ddots & \vdots \\
    b_{k1} & b_{k2} & \cdots & b_{kN_f}
    \end{bmatrix}%
  }
\]

Recall that \textbf{W}\textsuperscript{t} has rows equal to the number of documents and columns equaling the topics we have to choose from. Here, if the values for $a_{22}$ and $a_{23}$ are equal to one, then we know that the topics associated with column 2 and 3 perfectly describe document 2. To obtain the values that fill \textbf{W}\textsuperscript{t} we decompose \textbf{H}\textsuperscript{t} with relation to \textbf{X}\textsuperscript{t}. This process is the same NMF we used to describe The Badge Model in section 4. The matrix \textbf{U}\textsuperscript{t} is used to decompose a matrix similar to \textbf{H}\textsuperscript{t} in order to relate documents to users instead of textual features.

\begin{equation}
	\textrm{\textbf{U}}^t\approx \textrm{\textbf{W}}^t\textrm{\textbf{G}}^t 
\label{eq:featuresMatix}
\end{equation}

LTECS uses collective factorization to find a \textbf{W}\textsuperscript{t} that fulfills both the trained data from \textbf{G}\textsuperscript{t} and \textbf{H}\textsuperscript{t}.

%\section{Results of LTECS}
%\label{sec:topicEvolution}

%\section{Conclusion}
%\label{sec:conclusion}

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
% sample_paper.bib is the name of the BibTex file containing the
% bibliography entries. Note that you *don't* include the .bib ending here.
\bibliography{sample_paper}  
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}
